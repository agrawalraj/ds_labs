# ds_labs

### Overview 
This material is partly adapted from Mike Bernico's CS 570 class at UIS https://github.com/mbernico/CS570. Each lab
focuses on a few ideas, consisting of a mix of videos, notes, and programming exercises. There are also optional notes
that explain why different algorithms work/converge but sometimes require more statistics knowledge. Again no lab depends explicitly on this optional material. Hopefully, after working through these labs, you will feel comfortable with building really cool models! (TODO Make more exiting)

### Labs

Lab 1:
- Set up workstation
- Download relevant packages
- High-Level overview of Data Science 
- EDA 
- Math and statistics refresher 

Lab 2:
- Basic intro to decision theory 
- Baysian vs. Frequentist paradigms 
- Supervised vs. Unsupervised learning 
- Classification vs. Regression 

Lab 3:
- Bias-Variance tradeoff
- Non-parametric vs Parametric models 
- No Free Lunch Theorem
- High-level overview of model-building pipeline 

Lab 4:
- Lasso, OLS, Ridge Regression  
- Gradient Decent 
- Generaliztion errors, Cross-Validation, regularization  
- (Optional) theoretical properties of Lasso  

Lab 5:
- Logistic Regression 
- K-NN
- General Linear Models (GLMs) 

Lab 6:
- Decision Trees 
- Bootstrapping 
- Random Forests 

Lab 7:
- Curse of Dimensionality 
- Dimensionality Reduction 
- PCA
- JL Algorithm 
- (Optional) Concentration of measure 
- (Optional) theoretical properties of PCA and JL Algorithm

Lab 8: 
- Kernel Methods
- Representor theorem 
- How (almost) any estimator is kernelizeable   
- (Optional) Reproducing Kernel Hilbert Spaces (RKHS)

Lab 9:
- Non-parametrics 
- Connection with kernel methods
- Formalize past work 
- (Optional) Empirical risk minimization
- (Optional) Gaussian-Processes

Lab 10:
- Dealing with unstructured data 
- Bag-of-words, TFIDF
- LSA
- LDA 
- (Optional) Variational Bayes, MCMC, Dirchelet Processes 
